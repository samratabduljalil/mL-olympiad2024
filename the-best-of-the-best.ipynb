{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f8ec38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T10:49:08.626134Z",
     "iopub.status.busy": "2024-03-22T10:49:08.625726Z",
     "iopub.status.idle": "2024-03-22T10:49:11.451599Z",
     "shell.execute_reply": "2024-03-22T10:49:11.450681Z"
    },
    "papermill": {
     "duration": 2.834728,
     "end_time": "2024-03-22T10:49:11.454262",
     "exception": false,
     "start_time": "2024-03-22T10:49:08.619534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "Train=pd.read_csv('/kaggle/input/ml-olympiad-co2-emissions-prediction-challenge/train.csv')\n",
    "Test= pd.read_csv('/kaggle/input/ml-olympiad-co2-emissions-prediction-challenge/test.csv')\n",
    "sample_submission= pd.read_csv('/kaggle/input/subissino/submission (28) (1).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f58f27f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T10:49:11.462942Z",
     "iopub.status.busy": "2024-03-22T10:49:11.462356Z",
     "iopub.status.idle": "2024-03-22T10:49:11.484958Z",
     "shell.execute_reply": "2024-03-22T10:49:11.483617Z"
    },
    "papermill": {
     "duration": 0.029755,
     "end_time": "2024-03-22T10:49:11.487554",
     "exception": false,
     "start_time": "2024-03-22T10:49:11.457799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Train=Train.replace('..', .01020)  \n",
    "Test=Test.replace('..', .01020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bc17bfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T10:49:11.495925Z",
     "iopub.status.busy": "2024-03-22T10:49:11.495327Z",
     "iopub.status.idle": "2024-03-22T10:49:11.513647Z",
     "shell.execute_reply": "2024-03-22T10:49:11.512582Z"
    },
    "papermill": {
     "duration": 0.025333,
     "end_time": "2024-03-22T10:49:11.516177",
     "exception": false,
     "start_time": "2024-03-22T10:49:11.490844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Unique country for iteration\n",
    "unique = Train['Country Name'].unique()\n",
    "j= pd.read_csv('/kaggle/input/missing-prediction/data.csv')\n",
    "missing_value2 = j.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d584e341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T10:49:11.525064Z",
     "iopub.status.busy": "2024-03-22T10:49:11.524493Z",
     "iopub.status.idle": "2024-03-22T10:49:27.328891Z",
     "shell.execute_reply": "2024-03-22T10:49:27.327881Z"
    },
    "papermill": {
     "duration": 15.811744,
     "end_time": "2024-03-22T10:49:27.331492",
     "exception": false,
     "start_time": "2024-03-22T10:49:11.519748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 10:49:13.926838: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-22 10:49:13.926991: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-22 10:49:14.099154: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "#trainig and inferencing with LogisticRegression and ARIMA\n",
    "import pandas as pd\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "j=[]\n",
    "for i in unique:\n",
    "    \n",
    "    \n",
    "    fgt = Test[Test['Country Name'] == i].reset_index(drop=True)\n",
    "    columns_to_drop = [ 'Country Name','Indicator']\n",
    "    fgt = fgt.drop(columns_to_drop, axis=1)\n",
    "    transposed_df = fgt.transpose()\n",
    "    Xtest = transposed_df[[0,1, 2,3,4,9,10]]\n",
    "    transposed_df=0\n",
    "    fg = Train[Train['Country Name'] == i].reset_index(drop=True)\n",
    "    columns_to_drop = ['Country Code', 'Country Name','Indicator']\n",
    "    fg = fg.drop(columns_to_drop, axis=1)\n",
    "    transposed_df = fg.transpose()\n",
    "    #print(fg)\n",
    "    #print('done for now')\n",
    "    Xtrain = transposed_df[[0,1, 2,3,4,9,10]].astype(float)\n",
    "    xt=Xtrain.values\n",
    "    Ytrain=transposed_df[11].astype(float)\n",
    "    \n",
    "    try:\n",
    "        checkpoint_filepath = '/kaggle/working/best_model.keras'\n",
    "\n",
    "        # Define the ModelCheckpoint callback\n",
    "        checkpoint = ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                             monitor='val_loss',  # Monitor validation loss\n",
    "                             mode='min',         # Mode: minimize the monitored quantity\n",
    "                             save_best_only=True,  # Save only the best model\n",
    "                             # Save only the model weights\n",
    "                             verbose=1)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(xt, Ytrain, test_size=0.2, random_state=42)\n",
    "        loaded_model = tf.keras.models.load_model(\"/kaggle/input/deep-v4-22/my_lstm_model (4).h5\")\n",
    "        loaded_model.compile(optimizer = tf.keras.optimizers.Adam(), loss=tf.keras.losses.MeanSquaredError())\n",
    "        loaded_model.fit(X_train, y_train,validation_data=(X_test, y_test), epochs=200, batch_size=2,callbacks=[checkpoint])\n",
    "        \n",
    "        loaded_model2 = tf.keras.models.load_model(\"/kaggle/working/best_model.keras\")\n",
    "        forecast = loaded_model2.predict(Xtest.astype(float))\n",
    "        df = pd.DataFrame(forecast)\n",
    "        model2 = ARIMA(Ytrain.astype(float), order=(0,1,0))  # Adjust order as needed\n",
    "        model_fit2 = model2.fit()\n",
    "        print(forecast)\n",
    "    \n",
    "        forecast2 = model_fit2.forecast(steps=16)\n",
    "       \n",
    "        row_index = sample_submission[sample_submission.eq(i).any(axis=1)].index[0]\n",
    "\n",
    "\n",
    "        new_values = [i, forecast[0][0], forecast[1][0],forecast[2][0], forecast[3][0], forecast[4][0],forecast2[30]]\n",
    "        sample_submission.loc[row_index] = new_values\n",
    "    except:\n",
    "        j.append(i)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1ec6b22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T10:49:27.340886Z",
     "iopub.status.busy": "2024-03-22T10:49:27.340173Z",
     "iopub.status.idle": "2024-03-22T10:49:46.882464Z",
     "shell.execute_reply": "2024-03-22T10:49:46.881414Z"
    },
    "papermill": {
     "duration": 19.550245,
     "end_time": "2024-03-22T10:49:46.885228",
     "exception": false,
     "start_time": "2024-03-22T10:49:27.334983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 516ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 427ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step\n"
     ]
    }
   ],
   "source": [
    "#trainig and inferencing with LogisticRegression and ARIMA\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "for i in missing_value2:\n",
    "    \n",
    "    \n",
    "    fgt = Test[Test['Country Name'] == i[0]].reset_index(drop=True)\n",
    "    columns_to_drop = [ 'Country Name','Indicator']\n",
    "    fgt = fgt.drop(columns_to_drop, axis=1)\n",
    "    transposed_df = fgt.transpose()\n",
    "    Xtest = transposed_df[[0,1, 2,3,4,9,10]]\n",
    "    transposed_df=0\n",
    "    fg = Train[Train['Country Name'] == i[0]].reset_index(drop=True)\n",
    "    columns_to_drop = ['Country Code', 'Country Name','Indicator']\n",
    "    fg = fg.drop(columns_to_drop, axis=1)\n",
    "    transposed_df = fg.transpose()\n",
    "    #print(fg)\n",
    "    #print('done for now')\n",
    "    Xtrain = transposed_df[[0,1, 2,3,4,9,10]].astype(float)\n",
    "    xt=Xtrain.values\n",
    "    Ytrain=transposed_df[11].astype(float)\n",
    "    \n",
    "    \n",
    "    loaded_model = tf.keras.models.load_model(\"/kaggle/input/deep-v4-22/my_lstm_model (4).h5\")\n",
    "   \n",
    "    forecast = loaded_model.predict(Xtest.astype(float))\n",
    "    df = pd.DataFrame(forecast)\n",
    "    model2 = ARIMA(df[0], order=(0,1,0))  # Adjust order as needed\n",
    "    model_fit = model2.fit()\n",
    "\n",
    "    \n",
    "    forecast2 = model_fit.forecast(steps=16)\n",
    "        \n",
    "    row_index = sample_submission[sample_submission.eq(i[0]).any(axis=1)].index[0]\n",
    "\n",
    "\n",
    "    new_values = [i[0], forecast[0][0], forecast[1][0],forecast[2][0], forecast[3][0], forecast[4][0],forecast2[15]]\n",
    "    sample_submission.loc[row_index] = new_values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05dcd963",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T10:49:46.905106Z",
     "iopub.status.busy": "2024-03-22T10:49:46.904235Z",
     "iopub.status.idle": "2024-03-22T10:49:46.917433Z",
     "shell.execute_reply": "2024-03-22T10:49:46.916530Z"
    },
    "papermill": {
     "duration": 0.026441,
     "end_time": "2024-03-22T10:49:46.920456",
     "exception": false,
     "start_time": "2024-03-22T10:49:46.894015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7794824,
     "sourceId": 70983,
     "sourceType": "competition"
    },
    {
     "datasetId": 4601570,
     "sourceId": 7847571,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4611514,
     "sourceId": 7861213,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4619110,
     "sourceId": 7871978,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42.949589,
   "end_time": "2024-03-22T10:49:48.555774",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-22T10:49:05.606185",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
